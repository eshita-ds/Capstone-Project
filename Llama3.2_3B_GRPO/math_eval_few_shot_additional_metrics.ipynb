{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import evaluate\n",
                "\n",
                "from tqdm import tqdm"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "shot0_results = pd.read_csv(\"Mistral-7B-Instruct-v0.3_MATH_0_shot_test_results.csv\")\n",
                "shot1_results = pd.read_csv(\"Mistral-7B-Instruct-v0.3_MATH_1_shot_test_results.csv\")\n",
                "shot3_results = pd.read_csv(\"Mistral-7B-Instruct-v0.3_MATH_3_shot_test_results.csv\")\n",
                "shot5_results = pd.read_csv(\"Mistral-7B-Instruct-v0.3_MATH_5_shot_test_results.csv\")\n",
                "test_data = pd.read_csv(\"MATH_test_staging.csv\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Using the latest cached version of the module from /Users/nbthakur/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--meteor/e7ed321a1b44c34fa4679192809db2cee7e3bd4bba0fe8b76061d807706c2374 (last modified on Mon Oct 14 20:35:05 2024) since it couldn't be found locally at evaluate-metric--meteor, or remotely on the Hugging Face Hub.\n",
                        "[nltk_data] Downloading package wordnet to /Users/nbthakur/nltk_data...\n",
                        "[nltk_data]   Package wordnet is already up-to-date!\n",
                        "[nltk_data] Downloading package punkt_tab to /Users/nbthakur/nltk_data...\n",
                        "[nltk_data]   Package punkt_tab is already up-to-date!\n",
                        "[nltk_data] Downloading package omw-1.4 to /Users/nbthakur/nltk_data...\n",
                        "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
                        "Using the latest cached version of the module from /Users/nbthakur/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--chrf/d244bab9383988714085a8dacc4871986d9f025398581c33d6b2ee22836b4069 (last modified on Mon Oct 14 20:48:48 2024) since it couldn't be found locally at evaluate-metric--chrf, or remotely on the Hugging Face Hub.\n"
                    ]
                }
            ],
            "source": [
                "METEOR = evaluate.load(\"meteor\")\n",
                "CHRF = evaluate.load(\"chrf\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "evaluating: 100%|██████████| 100/100 [00:02<00:00, 34.53it/s]\n"
                    ]
                }
            ],
            "source": [
                "meteor_results = []\n",
                "chrf_results = []\n",
                "for idx in tqdm(range(100), desc=\"evaluating\"):\n",
                "    test_row = test_data.loc[idx]\n",
                "    sample_row = shot0_results.loc[idx]\n",
                "    meteor = METEOR.compute(\n",
                "        predictions=[sample_row[\"response\"]],\n",
                "        references=[[test_row[\"reasoning\"], test_row[\"answer\"]]],\n",
                "    )\n",
                "    chrf = CHRF.compute(\n",
                "        predictions=[sample_row[\"response\"]],\n",
                "        references=[[test_row[\"reasoning\"], test_row[\"answer\"]]],\n",
                "    )\n",
                "    meteor_results.append(float(meteor[\"meteor\"]))\n",
                "    chrf_results.append(chrf[\"score\"])\n",
                "shot0_results[\"meteor\"] = meteor_results\n",
                "shot0_results[\"chrf\"] = chrf_results\n",
                "shot0_results[\n",
                "    [\n",
                "        \"dataset_id\",\n",
                "        \"question_id\",\n",
                "        \"bleu\",\n",
                "        \"rouge1\",\n",
                "        \"rouge2\",\n",
                "        \"rougeL\",\n",
                "        \"meteor\",\n",
                "        \"chrf\",\n",
                "        \"response\",\n",
                "    ]\n",
                "].to_csv(\"Mistral-7B-Instruct-v0.3_MATH_0_shot_test_results.csv\", index=False)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "evaluating: 100%|██████████| 100/100 [00:01<00:00, 78.19it/s]\n"
                    ]
                }
            ],
            "source": [
                "meteor_results = []\n",
                "chrf_results = []\n",
                "for idx in tqdm(range(100), desc=\"evaluating\"):\n",
                "    test_row = test_data.loc[idx]\n",
                "    sample_row = shot1_results.loc[idx]\n",
                "    meteor = METEOR.compute(\n",
                "        predictions=[sample_row[\"response\"]],\n",
                "        references=[[test_row[\"reasoning\"], test_row[\"answer\"]]],\n",
                "    )\n",
                "    chrf = CHRF.compute(\n",
                "        predictions=[sample_row[\"response\"]],\n",
                "        references=[[test_row[\"reasoning\"], test_row[\"answer\"]]],\n",
                "    )\n",
                "    meteor_results.append(float(meteor[\"meteor\"]))\n",
                "    chrf_results.append(chrf[\"score\"])\n",
                "shot1_results[\"meteor\"] = meteor_results\n",
                "shot1_results[\"chrf\"] = chrf_results\n",
                "shot1_results[\n",
                "    [\n",
                "        \"dataset_id\",\n",
                "        \"question_id\",\n",
                "        \"bleu\",\n",
                "        \"rouge1\",\n",
                "        \"rouge2\",\n",
                "        \"rougeL\",\n",
                "        \"meteor\",\n",
                "        \"chrf\",\n",
                "        \"response\",\n",
                "    ]\n",
                "].to_csv(\"Mistral-7B-Instruct-v0.3_MATH_1_shot_test_results.csv\", index=False)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "evaluating: 100%|██████████| 100/100 [00:01<00:00, 86.64it/s]\n"
                    ]
                }
            ],
            "source": [
                "meteor_results = []\n",
                "chrf_results = []\n",
                "for idx in tqdm(range(100), desc=\"evaluating\"):\n",
                "    test_row = test_data.loc[idx]\n",
                "    sample_row = shot3_results.loc[idx]\n",
                "    meteor = METEOR.compute(\n",
                "        predictions=[sample_row[\"response\"]],\n",
                "        references=[[test_row[\"reasoning\"], test_row[\"answer\"]]],\n",
                "    )\n",
                "    chrf = CHRF.compute(\n",
                "        predictions=[sample_row[\"response\"]],\n",
                "        references=[[test_row[\"reasoning\"], test_row[\"answer\"]]],\n",
                "    )\n",
                "    meteor_results.append(float(meteor[\"meteor\"]))\n",
                "    chrf_results.append(chrf[\"score\"])\n",
                "shot3_results[\"meteor\"] = meteor_results\n",
                "shot3_results[\"chrf\"] = chrf_results\n",
                "shot3_results[\n",
                "    [\n",
                "        \"dataset_id\",\n",
                "        \"question_id\",\n",
                "        \"bleu\",\n",
                "        \"rouge1\",\n",
                "        \"rouge2\",\n",
                "        \"rougeL\",\n",
                "        \"meteor\",\n",
                "        \"chrf\",\n",
                "        \"response\",\n",
                "    ]\n",
                "].to_csv(\"Mistral-7B-Instruct-v0.3_MATH_3_shot_test_results.csv\", index=False)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "evaluating: 100%|██████████| 100/100 [00:01<00:00, 87.96it/s]\n"
                    ]
                }
            ],
            "source": [
                "meteor_results = []\n",
                "chrf_results = []\n",
                "for idx in tqdm(range(100), desc=\"evaluating\"):\n",
                "    test_row = test_data.loc[idx]\n",
                "    sample_row = shot5_results.loc[idx]\n",
                "    meteor = METEOR.compute(\n",
                "        predictions=[sample_row[\"response\"]],\n",
                "        references=[[test_row[\"reasoning\"], test_row[\"answer\"]]],\n",
                "    )\n",
                "    chrf = CHRF.compute(\n",
                "        predictions=[sample_row[\"response\"]],\n",
                "        references=[[test_row[\"reasoning\"], test_row[\"answer\"]]],\n",
                "    )\n",
                "    meteor_results.append(float(meteor[\"meteor\"]))\n",
                "    chrf_results.append(chrf[\"score\"])\n",
                "shot5_results[\"meteor\"] = meteor_results\n",
                "shot5_results[\"chrf\"] = chrf_results\n",
                "shot5_results[\n",
                "    [\n",
                "        \"dataset_id\",\n",
                "        \"question_id\",\n",
                "        \"bleu\",\n",
                "        \"rouge1\",\n",
                "        \"rouge2\",\n",
                "        \"rougeL\",\n",
                "        \"meteor\",\n",
                "        \"chrf\",\n",
                "        \"response\",\n",
                "    ]\n",
                "].to_csv(\"Mistral-7B-Instruct-v0.3_MATH_5_shot_test_results.csv\", index=False)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.8"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}