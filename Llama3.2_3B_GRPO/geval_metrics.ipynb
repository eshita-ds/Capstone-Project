{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "e84c2362",
            "metadata": {},
            "outputs": [],
            "source": [
                "from deepeval.test_case import LLMTestCase, LLMTestCaseParams\n",
                "from deepeval.metrics import GEval\n",
                "from dotenv import load_dotenv\n",
                "from tqdm import tqdm\n",
                "\n",
                "import pandas as pd\n",
                "import time"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "7e7f821f",
            "metadata": {},
            "outputs": [],
            "source": [
                "_ = load_dotenv()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "d963e411",
            "metadata": {},
            "outputs": [],
            "source": [
                "shot0_results = pd.read_csv(\"Llama-3.2-3B-Instruct_MATH_0_shot_test_results.csv\")\n",
                "reasn_results = pd.read_csv(\"Llama-3.2-3B-Instruct_MATH_cot_reasoning_test_results.csv\")\n",
                "test_data = pd.read_csv(\"MATH_test_staging.csv\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "9168231d",
            "metadata": {},
            "outputs": [],
            "source": [
                "correctness_metric = GEval(\n",
                "    name=\"Correctness\",\n",
                "    criteria=\"Determine whether the actual output is factually correct based on the expected output.\",\n",
                "    evaluation_params=[\n",
                "        LLMTestCaseParams.INPUT,\n",
                "        LLMTestCaseParams.ACTUAL_OUTPUT,\n",
                "        LLMTestCaseParams.EXPECTED_OUTPUT,\n",
                "    ],\n",
                "    model=\"gpt-4o\",\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f34cf0c5",
            "metadata": {},
            "outputs": [],
            "source": [
                "logical_metric = GEval(\n",
                "    name=\"Logical Coherence\",\n",
                "    criteria=\"Determine whether the steps followed in actual output are logically correct and coherent based on the expected output. Whether the final answer matches is not important.\",\n",
                "    evaluation_params=[\n",
                "        LLMTestCaseParams.INPUT,\n",
                "        LLMTestCaseParams.ACTUAL_OUTPUT,\n",
                "        LLMTestCaseParams.EXPECTED_OUTPUT,\n",
                "    ],\n",
                "    model=\"gpt-4o\",\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "96fcf83b",
            "metadata": {},
            "outputs": [],
            "source": [
                "geval_score = GEval(\n",
                "    name=\"Reasoning Quality\",\n",
                "    criteria=\"Evaluate the chain-of-thought reasoning in the actual output compared to the expected output. Assess whether the reasoning steps are logical, complete, and lead to the correct conclusion.\",\n",
                "    evaluation_steps=[\n",
                "        \"Check if each reasoning step in the actual output is explicitly stated and follows logically from the previous step.\",\n",
                "        \"Verify that all critical steps from the expected output are present in the actual output.\",\n",
                "        \"Determine if the final conclusion in the actual output matches the expected outout and is supported by the reasoning.\"\n",
                "    ],\n",
                "    evaluation_params=[\n",
                "        LLMTestCaseParams.INPUT,\n",
                "        LLMTestCaseParams.ACTUAL_OUTPUT,\n",
                "        LLMTestCaseParams.EXPECTED_OUTPUT,\n",
                "    ],\n",
                "    model=\"gpt-4o\",\n",
                "    threshold=0.7,\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "7a4dc59f",
            "metadata": {},
            "outputs": [],
            "source": [
                "dataset_id = []\n",
                "question_id = []\n",
                "correctness_score = []\n",
                "correctness_reason = []\n",
                "logical_score = []\n",
                "logical_reason = []"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "9b568035",
            "metadata": {},
            "outputs": [],
            "source": [
                "for idx in tqdm(range(100), desc=\"evaluating\"):\n",
                "    test_row = test_data.loc[idx]\n",
                "    sample_row = shot0_results.loc[idx]\n",
                "    testcase = LLMTestCase(\n",
                "        input=test_row[\"question_text\"],\n",
                "        actual_output=sample_row[\"response\"],\n",
                "        expected_output=test_row[\"reasoning\"],\n",
                "    )\n",
                "    while True:\n",
                "        try:\n",
                "            _ = correctness_metric.measure(testcase, _show_indicator=False)\n",
                "            break\n",
                "        except:\n",
                "            time.sleep(2)\n",
                "    correctness_score.append(correctness_metric.score)\n",
                "    correctness_reason.append(correctness_metric.reason)\n",
                "    time.sleep(2)\n",
                "    while True:\n",
                "        try:\n",
                "            _ = logical_metric.measure(testcase, _show_indicator=False)\n",
                "            break\n",
                "        except:\n",
                "            time.sleep(2)\n",
                "    logical_score.append(logical_metric.score)\n",
                "    logical_reason.append(logical_metric.reason)\n",
                "    dataset_id.append(test_row[\"dataset_id\"])\n",
                "    question_id.append(test_row[\"question_id\"])\n",
                "    time.sleep(4)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "34feb49c",
            "metadata": {},
            "outputs": [],
            "source": [
                "pd.DataFrame(\n",
                "    {\n",
                "        \"dataset_id\": dataset_id,\n",
                "        \"question_id\": question_id,\n",
                "        \"geval_correctness\": correctness_score,\n",
                "        \"geval_correctness_trace\": correctness_reason,\n",
                "        \"geval_logical\": logical_score,\n",
                "        \"geval_logical_trace\": logical_reason,\n",
                "    }\n",
                ").to_csv(\"Llama-3.2-3B-Instruct_MATH_0_shot_test_geval.csv\", index=False)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "4d66a39d",
            "metadata": {},
            "outputs": [],
            "source": [
                "dataset_id = []\n",
                "question_id = []\n",
                "correctness_score = []\n",
                "correctness_reason = []\n",
                "logical_score = []\n",
                "logical_reason = []"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "e74a34d0",
            "metadata": {},
            "outputs": [],
            "source": [
                "for idx in tqdm(range(100), desc=\"evaluating\"):\n",
                "    test_row = test_data.loc[idx]\n",
                "    sample_row = reasn_results.loc[idx]\n",
                "    dataset_id.append(test_row[\"dataset_id\"])\n",
                "    question_id.append(test_row[\"question_id\"])\n",
                "    testcase = LLMTestCase(\n",
                "        input=test_row[\"question_text\"],\n",
                "        actual_output=sample_row[\"response\"],\n",
                "        expected_output=test_row[\"reasoning\"],\n",
                "    )\n",
                "    while True:\n",
                "        try:\n",
                "            _ = correctness_metric.measure(testcase, _show_indicator=False)\n",
                "            break\n",
                "        except:\n",
                "            time.sleep(2)\n",
                "    correctness_score.append(correctness_metric.score)\n",
                "    correctness_reason.append(correctness_metric.reason)\n",
                "    time.sleep(2)\n",
                "    while True:\n",
                "        try:\n",
                "            _ = logical_metric.measure(testcase, _show_indicator=False)\n",
                "            break\n",
                "        except:\n",
                "            time.sleep(2)\n",
                "    logical_score.append(logical_metric.score)\n",
                "    logical_reason.append(logical_metric.reason)\n",
                "    time.sleep(4)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "4bc5f993",
            "metadata": {},
            "outputs": [],
            "source": [
                "pd.DataFrame(\n",
                "    {\n",
                "        \"dataset_id\": dataset_id,\n",
                "        \"question_id\": question_id,\n",
                "        \"geval_correctness\": correctness_score,\n",
                "        \"geval_correctness_trace\": correctness_reason,\n",
                "        \"geval_logical\": logical_score,\n",
                "        \"geval_logical_trace\": logical_reason,\n",
                "    }\n",
                ").to_csv(\"Llama-3.2-3B-Instruct_MATH_cot_reasoning_test_geval.csv\", index=False)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.8"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}