{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c07403f-13e9-48cf-a363-3ae04f1de5f1",
   "metadata": {
    "id": "9c07403f-13e9-48cf-a363-3ae04f1de5f1",
    "outputId": "9990e91c-7051-42e7-83ba-ea8d0b6190bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.51.3)\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (3.6.0)\n",
      "Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (1.6.0)\n",
      "Requirement already satisfied: bitsandbytes in /opt/conda/lib/python3.10/site-packages (0.45.5)\n",
      "Requirement already satisfied: peft in /opt/conda/lib/python3.10/site-packages (0.15.2)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.31.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2023.12.2)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.1.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.18)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2025.4.26)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (4.0.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=2.0.0->accelerate) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=2.0.0->accelerate) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (0.2.0)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.1.2)\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (0.16.2)\n",
      "Requirement already satisfied: torchaudio in /opt/conda/lib/python3.10/site-packages (2.1.2)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2023.12.2)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision) (1.26.2)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision) (2.32.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision) (10.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (2025.4.26)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: huggingface_hub in /opt/conda/lib/python3.10/site-packages (0.31.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (2023.12.2)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (6.0.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (4.13.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (1.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (2025.4.26)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: langchain in /opt/conda/lib/python3.10/site-packages (0.3.25)\n",
      "Requirement already satisfied: llama-index in /opt/conda/lib/python3.10/site-packages (0.12.34)\n",
      "Requirement already satisfied: pypdf in /opt/conda/lib/python3.10/site-packages (5.4.0)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.58 in /opt/conda/lib/python3.10/site-packages (from langchain) (0.3.59)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /opt/conda/lib/python3.10/site-packages (from langchain) (0.3.8)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /opt/conda/lib/python3.10/site-packages (from langchain) (0.3.42)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.11.4)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.0.40)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.10/site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: llama-index-agent-openai<0.5,>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from llama-index) (0.4.7)\n",
      "Requirement already satisfied: llama-index-cli<0.5,>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from llama-index) (0.4.1)\n",
      "Requirement already satisfied: llama-index-core<0.13,>=0.12.34 in /opt/conda/lib/python3.10/site-packages (from llama-index) (0.12.34.post1)\n",
      "Requirement already satisfied: llama-index-embeddings-openai<0.4,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from llama-index) (0.3.1)\n",
      "Requirement already satisfied: llama-index-indices-managed-llama-cloud>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from llama-index) (0.6.11)\n",
      "Requirement already satisfied: llama-index-llms-openai<0.4,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from llama-index) (0.3.38)\n",
      "Requirement already satisfied: llama-index-multi-modal-llms-openai<0.5,>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from llama-index) (0.4.3)\n",
      "Requirement already satisfied: llama-index-program-openai<0.4,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from llama-index) (0.3.1)\n",
      "Requirement already satisfied: llama-index-question-gen-openai<0.4,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from llama-index) (0.3.0)\n",
      "Requirement already satisfied: llama-index-readers-file<0.5,>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from llama-index) (0.4.7)\n",
      "Requirement already satisfied: llama-index-readers-llama-parse>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from llama-index) (0.4.0)\n",
      "Requirement already satisfied: nltk>3.8.1 in /opt/conda/lib/python3.10/site-packages (from llama-index) (3.9.1)\n",
      "Requirement already satisfied: typing_extensions>=4.0 in /opt/conda/lib/python3.10/site-packages (from pypdf) (4.13.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.10/site-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /opt/conda/lib/python3.10/site-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (24.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/conda/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.18)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: openai>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-agent-openai<0.5,>=0.4.0->llama-index) (1.77.0)\n",
      "Requirement already satisfied: aiohttp<4,>=3.8.6 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.13,>=0.12.34->llama-index) (3.11.18)\n",
      "Requirement already satisfied: banks<3,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.13,>=0.12.34->llama-index) (2.1.2)\n",
      "Requirement already satisfied: dataclasses-json in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.13,>=0.12.34->llama-index) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.13,>=0.12.34->llama-index) (1.2.18)\n",
      "Requirement already satisfied: dirtyjson<2,>=1.0.8 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.13,>=0.12.34->llama-index) (1.0.8)\n",
      "Requirement already satisfied: filetype<2,>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.13,>=0.12.34->llama-index) (1.2.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.13,>=0.12.34->llama-index) (2023.12.2)\n",
      "Requirement already satisfied: nest-asyncio<2,>=1.5.8 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.13,>=0.12.34->llama-index) (1.5.8)\n",
      "Requirement already satisfied: networkx>=3.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.13,>=0.12.34->llama-index) (3.1)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.13,>=0.12.34->llama-index) (1.26.2)\n",
      "Requirement already satisfied: pillow>=9.0.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.13,>=0.12.34->llama-index) (10.0.1)\n",
      "Requirement already satisfied: tiktoken>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.13,>=0.12.34->llama-index) (0.9.0)\n",
      "Requirement already satisfied: tqdm<5,>=4.66.1 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.13,>=0.12.34->llama-index) (4.67.1)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.13,>=0.12.34->llama-index) (0.9.0)\n",
      "Requirement already satisfied: wrapt in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.13,>=0.12.34->llama-index) (1.17.2)\n",
      "Requirement already satisfied: llama-cloud<0.2.0,>=0.1.13 in /opt/conda/lib/python3.10/site-packages (from llama-index-indices-managed-llama-cloud>=0.4.0->llama-index) (0.1.19)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /opt/conda/lib/python3.10/site-packages (from llama-index-readers-file<0.5,>=0.4.0->llama-index) (4.13.4)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from llama-index-readers-file<0.5,>=0.4.0->llama-index) (2.2.3)\n",
      "Requirement already satisfied: striprtf<0.0.27,>=0.0.26 in /opt/conda/lib/python3.10/site-packages (from llama-index-readers-file<0.5,>=0.4.0->llama-index) (0.0.26)\n",
      "Requirement already satisfied: llama-parse>=0.5.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-readers-llama-parse>=0.4.0->llama-index) (0.6.21)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk>3.8.1->llama-index) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk>3.8.1->llama-index) (1.5.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.10/site-packages (from nltk>3.8.1->llama-index) (2024.11.6)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /opt/conda/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2025.4.26)\n",
      "Requirement already satisfied: greenlet>=1 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.34->llama-index) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.34->llama-index) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.34->llama-index) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.34->llama-index) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.34->llama-index) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.34->llama-index) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.34->llama-index) (1.20.0)\n",
      "Requirement already satisfied: griffe in /opt/conda/lib/python3.10/site-packages (from banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.34->llama-index) (1.7.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.34->llama-index) (3.1.2)\n",
      "Requirement already satisfied: platformdirs in /opt/conda/lib/python3.10/site-packages (from banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.34->llama-index) (4.3.8)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.5,>=0.4.0->llama-index) (2.5)\n",
      "Requirement already satisfied: anyio in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.1.0)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.16.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.58->langchain) (2.1)\n",
      "Requirement already satisfied: llama-cloud-services>=0.6.21 in /opt/conda/lib/python3.10/site-packages (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index) (0.6.21)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/conda/lib/python3.10/site-packages (from openai>=1.14.0->llama-index-agent-openai<0.5,>=0.4.0->llama-index) (1.8.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from openai>=1.14.0->llama-index-agent-openai<0.5,>=0.4.0->llama-index) (0.9.0)\n",
      "Requirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from openai>=1.14.0->llama-index-agent-openai<0.5,>=0.4.0->llama-index) (1.3.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.13,>=0.12.34->llama-index) (1.1.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json->llama-index-core<0.13,>=0.12.34->llama-index) (3.26.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->llama-index-readers-file<0.5,>=0.4.0->llama-index) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->llama-index-readers-file<0.5,>=0.4.0->llama-index) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->llama-index-readers-file<0.5,>=0.4.0->llama-index) (2025.2)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.4)\n",
      "Requirement already satisfied: python-dotenv<2.0.0,>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from llama-cloud-services>=0.6.21->llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index) (1.1.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->llama-index-readers-file<0.5,>=0.4.0->llama-index) (1.16.0)\n",
      "Requirement already satisfied: colorama>=0.4 in /opt/conda/lib/python3.10/site-packages (from griffe->banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.34->llama-index) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.34->llama-index) (2.1.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: langchain-community in /opt/conda/lib/python3.10/site-packages (0.3.23)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.56 in /opt/conda/lib/python3.10/site-packages (from langchain-community) (0.3.59)\n",
      "Requirement already satisfied: langchain<1.0.0,>=0.3.24 in /opt/conda/lib/python3.10/site-packages (from langchain-community) (0.3.25)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.10/site-packages (from langchain-community) (2.0.40)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.10/site-packages (from langchain-community) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.10/site-packages (from langchain-community) (6.0.1)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.10/site-packages (from langchain-community) (3.11.18)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain-community) (9.1.2)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/conda/lib/python3.10/site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /opt/conda/lib/python3.10/site-packages (from langchain-community) (2.9.1)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /opt/conda/lib/python3.10/site-packages (from langchain-community) (0.3.42)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from langchain-community) (0.4.0)\n",
      "Requirement already satisfied: numpy>=1.26.2 in /opt/conda/lib/python3.10/site-packages (from langchain-community) (1.26.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (4.0.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /opt/conda/lib/python3.10/site-packages (from langchain<1.0.0,>=0.3.24->langchain-community) (0.3.8)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /opt/conda/lib/python3.10/site-packages (from langchain<1.0.0,>=0.3.24->langchain-community) (2.11.4)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.10/site-packages (from langchain-core<1.0.0,>=0.3.56->langchain-community) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /opt/conda/lib/python3.10/site-packages (from langchain-core<1.0.0,>=0.3.56->langchain-community) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /opt/conda/lib/python3.10/site-packages (from langchain-core<1.0.0,>=0.3.56->langchain-community) (4.13.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/conda/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.125->langchain-community) (3.10.18)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.125->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.23.0)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.1.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (0.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain-community) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain-community) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain-community) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain-community) (2025.4.26)\n",
      "Requirement already satisfied: greenlet>=1 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.2.1)\n",
      "Requirement already satisfied: anyio in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (4.1.0)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (0.16.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.56->langchain-community) (2.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.24->langchain-community) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /opt/conda/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.24->langchain-community) (2.33.2)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.1.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.10/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.0.4)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: langchain in /opt/conda/lib/python3.10/site-packages (0.3.25)\n",
      "Requirement already satisfied: pypdf in /opt/conda/lib/python3.10/site-packages (5.4.0)\n",
      "Requirement already satisfied: chromadb in /opt/conda/lib/python3.10/site-packages (1.0.8)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.58 in /opt/conda/lib/python3.10/site-packages (from langchain) (0.3.59)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /opt/conda/lib/python3.10/site-packages (from langchain) (0.3.8)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /opt/conda/lib/python3.10/site-packages (from langchain) (0.3.42)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.11.4)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.0.40)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.10/site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: typing_extensions>=4.0 in /opt/conda/lib/python3.10/site-packages (from pypdf) (4.13.2)\n",
      "Requirement already satisfied: build>=1.0.3 in /opt/conda/lib/python3.10/site-packages (from chromadb) (1.2.2.post1)\n",
      "Requirement already satisfied: fastapi==0.115.9 in /opt/conda/lib/python3.10/site-packages (from chromadb) (0.115.9)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.34.2)\n",
      "Requirement already satisfied: numpy>=1.22.5 in /opt/conda/lib/python3.10/site-packages (from chromadb) (1.26.2)\n",
      "Requirement already satisfied: posthog>=2.4.0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (4.0.1)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in /opt/conda/lib/python3.10/site-packages (from chromadb) (1.21.1)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (1.32.1)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (1.32.1)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (0.53b1)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (1.32.1)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in /opt/conda/lib/python3.10/site-packages (from chromadb) (0.21.1)\n",
      "Requirement already satisfied: pypika>=0.48.9 in /opt/conda/lib/python3.10/site-packages (from chromadb) (0.48.9)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (4.67.1)\n",
      "Requirement already satisfied: overrides>=7.3.1 in /opt/conda/lib/python3.10/site-packages (from chromadb) (7.4.0)\n",
      "Requirement already satisfied: importlib-resources in /opt/conda/lib/python3.10/site-packages (from chromadb) (6.5.2)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (1.71.0)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from chromadb) (4.3.0)\n",
      "Requirement already satisfied: typer>=0.9.0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (0.15.3)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (32.0.1)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in /opt/conda/lib/python3.10/site-packages (from chromadb) (9.1.2)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from chromadb) (5.1.0)\n",
      "Requirement already satisfied: orjson>=3.9.12 in /opt/conda/lib/python3.10/site-packages (from chromadb) (3.10.18)\n",
      "Requirement already satisfied: httpx>=0.27.0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (0.28.1)\n",
      "Requirement already satisfied: rich>=10.11.0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (14.0.0)\n",
      "Requirement already satisfied: jsonschema>=4.19.0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (4.19.2)\n",
      "Requirement already satisfied: starlette<0.46.0,>=0.40.0 in /opt/conda/lib/python3.10/site-packages (from fastapi==0.115.9->chromadb) (0.45.3)\n",
      "Requirement already satisfied: packaging>=19.1 in /opt/conda/lib/python3.10/site-packages (from build>=1.0.3->chromadb) (24.2)\n",
      "Requirement already satisfied: pyproject_hooks in /opt/conda/lib/python3.10/site-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
      "Requirement already satisfied: tomli>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from build>=1.0.3->chromadb) (2.0.1)\n",
      "Requirement already satisfied: anyio in /opt/conda/lib/python3.10/site-packages (from httpx>=0.27.0->chromadb) (4.1.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx>=0.27.0->chromadb) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx>=0.27.0->chromadb) (1.0.9)\n",
      "Requirement already satisfied: idna in /opt/conda/lib/python3.10/site-packages (from httpx>=0.27.0->chromadb) (3.4)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.16.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=4.19.0->chromadb) (23.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=4.19.0->chromadb) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=4.19.0->chromadb) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=4.19.0->chromadb) (0.10.6)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (2.8.2)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (2.40.1)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (1.7.0)\n",
      "Requirement already satisfied: requests-oauthlib in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (1.26.18)\n",
      "Requirement already satisfied: durationpy>=0.7 in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (0.9)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.10/site-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (1.33)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: coloredlogs in /opt/conda/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /opt/conda/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\n",
      "Requirement already satisfied: protobuf in /opt/conda/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (5.29.4)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (1.12)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.18)\n",
      "Requirement already satisfied: importlib-metadata<8.7.0,>=6.0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-api>=1.2.0->chromadb) (8.6.1)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.70.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.32.1 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.32.1)\n",
      "Requirement already satisfied: opentelemetry-proto==1.32.1 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.32.1)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.53b1 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.53b1)\n",
      "Requirement already satisfied: opentelemetry-instrumentation==0.53b1 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.53b1)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.53b1 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.53b1)\n",
      "Requirement already satisfied: opentelemetry-util-http==0.53b1 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.53b1)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-instrumentation==0.53b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.17.2)\n",
      "Requirement already satisfied: asgiref~=3.0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-instrumentation-asgi==0.53b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (3.8.1)\n",
      "Requirement already satisfied: backoff>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: distro>=1.5.0 in /opt/conda/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb) (1.8.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /opt/conda/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2.0.4)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.11.0->chromadb) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.11.0->chromadb) (2.15.1)\n",
      "Requirement already satisfied: greenlet>=1 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /opt/conda/lib/python3.10/site-packages (from tokenizers>=0.13.2->chromadb) (0.31.1)\n",
      "Requirement already satisfied: click>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from typer>=0.9.0->chromadb) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
      "Requirement already satisfied: httptools>=0.6.3 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.4)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.0)\n",
      "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.5)\n",
      "Requirement already satisfied: websockets>=10.4 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2023.12.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (1.1.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata<8.7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.21.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.58->langchain) (2.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.10/site-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio->httpx>=0.27.0->chromadb) (1.0.4)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /opt/conda/lib/python3.10/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: sentence-transformers in /opt/conda/lib/python3.10/site-packages (4.1.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (4.51.3)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (2.1.2)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.15.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (0.31.1)\n",
      "Requirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (10.0.1)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (4.13.2)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2023.12.2)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (1.5.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.4.26)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets accelerate bitsandbytes peft\n",
    "!pip install sentencepiece torch torchvision torchaudio\n",
    "!pip install huggingface_hub\n",
    "!pip install langchain llama-index pypdf\n",
    "!pip install -U langchain-community\n",
    "!pip install langchain pypdf chromadb\n",
    "!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60a09964-5d24-453f-a573-0f384a22f9c7",
   "metadata": {
    "id": "60a09964-5d24-453f-a573-0f384a22f9c7"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.empty_cache()  # Clears unused memory\n",
    "torch.cuda.ipc_collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf843caa-b4aa-49c1-9574-7e731e77ba7d",
   "metadata": {
    "id": "bf843caa-b4aa-49c1-9574-7e731e77ba7d",
    "outputId": "a5ad5f03-d07c-43d4-b37c-b6c909cee120"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: peft in /opt/conda/lib/python3.10/site-packages (0.15.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft) (1.26.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft) (24.2)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft) (2.1.2)\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft) (4.51.3)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft) (4.67.1)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft) (1.6.0)\n",
      "Requirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft) (0.5.3)\n",
      "Requirement already satisfied: huggingface_hub>=0.25.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.31.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface_hub>=0.25.0->peft) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub>=0.25.0->peft) (2023.12.2)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface_hub>=0.25.0->peft) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub>=0.25.0->peft) (4.13.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub>=0.25.0->peft) (1.1.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (0.21.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub>=0.25.0->peft) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub>=0.25.0->peft) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub>=0.25.0->peft) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub>=0.25.0->peft) (2025.4.26)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f3aed06-b659-4374-9047-77629519622c",
   "metadata": {
    "id": "8f3aed06-b659-4374-9047-77629519622c",
    "outputId": "5dc47144-d9ba-4f77-d8fd-5fedb9781bdc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   dataset_id  question_id                                      question_text  \\\n",
      "0           2          427  Find the equation whose graph is a parabola wi...   \n",
      "1           2         1594  If $f(x)=\\sqrt{x-3}$, what is the smallest rea...   \n",
      "2           2          806  Positive real numbers $r,s$ satisfy the equati...   \n",
      "3           2          411  If $\\log_{25}(x-4)=\\frac{1}{2}$, find $\\frac{1...   \n",
      "4           2         6260  How many perfect squares are two-digit and div...   \n",
      "\n",
      "                                              answer  \\\n",
      "0  Hence the answer to this question is $-3x^2+12...   \n",
      "1         Hence the answer to this question is $12$.   \n",
      "2          Hence the answer to this question is $1$.   \n",
      "3          Hence the answer to this question is $2$.   \n",
      "4          Hence the answer to this question is $2$.   \n",
      "\n",
      "                                           reasoning  \n",
      "0  Since the axis of symmetry is vertical and the...  \n",
      "1  Note that $f(x)$ is defined if and only if $x\\...  \n",
      "2  We have $2r^2s^2 = (r^4 + 2r^2s^2 + s^4) - (r^...  \n",
      "3  First we solve for $x$. Converting our logarit...  \n",
      "4  Recall that no perfect squares are negative, b...  \n",
      "   dataset_id  question_id                                      question_text  \\\n",
      "0           2         2827  In the staircase-shaped region below, all angl...   \n",
      "1           2         6957  Below is the graph of $y = a \\sin (bx + c) + d...   \n",
      "2           2         3015  Let $ABCD$ be an isosceles trapezoid with $\\ov...   \n",
      "3           2         6570  Suppose that $x = 2-t$ and $y = 4t + 7$.  If $...   \n",
      "4           2         4607  Done with her new problems, Wendy takes a brea...   \n",
      "\n",
      "                                       answer  \\\n",
      "0  Hence the answer to this question is $32$.   \n",
      "1   Hence the answer to this question is $1$.   \n",
      "2  Hence the answer to this question is $32$.   \n",
      "3  Hence the answer to this question is $27$.   \n",
      "4   Hence the answer to this question is $1$.   \n",
      "\n",
      "                                           reasoning  \n",
      "0  We can look at the region as a rectangle with ...  \n",
      "1  The graph oscillates between 3 and $-1,$ so $d...  \n",
      "2  [asy] size(300); defaultpen(1); pair A=(0,0), ...  \n",
      "3  If $x=-3$, then $-3 = 2-t$, so $t = 5$.  There...  \n",
      "4  By Vieta's Formulas, given that $r_1, r_2, \\cd...  \n",
      "   dataset_id  question_id                                      question_text  \\\n",
      "0           2            1  How many vertical asymptotes does the graph of...   \n",
      "1           2            2  What is the positive difference between $120\\%...   \n",
      "2           2            3  Find $x$ such that $\\lceil x \\rceil + x = \\dfr...   \n",
      "3           2            4                     Evaluate $i^5+i^{-25}+i^{45}$.   \n",
      "4           2            5            If $2^8=4^x$, what is the value of $x$?   \n",
      "\n",
      "                                       answer  \\\n",
      "0   Hence the answer to this question is $2$.   \n",
      "1  Hence the answer to this question is $10$.   \n",
      "2   Hence the answer to this question is $9$.   \n",
      "3   Hence the answer to this question is $i$.   \n",
      "4   Hence the answer to this question is $4$.   \n",
      "\n",
      "                                           reasoning  \n",
      "0  The denominator of the rational function facto...  \n",
      "1  One hundred twenty percent of 30 is $120\\cdot3...  \n",
      "2  First, we note that $x$ must be positive, sinc...  \n",
      "3  We have $i^5 = i^4\\cdot i = 1\\cdot (i) = i$.  ...  \n",
      "4  Rewrite $4$ as $2^2$ to find $4^x=2^{2x}$.  Si...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "train_df = pd.read_csv('MATH_train_staging.csv')  # Update the path\n",
    "val_df = pd.read_csv('MATH_val_staging.csv')\n",
    "test_df = pd.read_csv('MATH_test_staging.csv')\n",
    "\n",
    "# Check structure\n",
    "print(train_df.head())\n",
    "print(val_df.head())\n",
    "print(test_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbd6a691-cc33-4339-9438-92b2208e7d68",
   "metadata": {
    "id": "cbd6a691-cc33-4339-9438-92b2208e7d68"
   },
   "outputs": [],
   "source": [
    "train_df.rename(columns={'question_text': 'question', 'reasoning': 'chain_of_thought', 'answer': 'answer'}, inplace=True)\n",
    "val_df.rename(columns={'question_text': 'question', 'reasoning': 'chain_of_thought', 'answer': 'answer'}, inplace=True)\n",
    "test_df.rename(columns={'question_text': 'question', 'reasoning': 'chain_of_thought', 'answer': 'answer'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5fb52ff6-58bf-4d5c-9b24-4f7db1ec9143",
   "metadata": {
    "id": "5fb52ff6-58bf-4d5c-9b24-4f7db1ec9143"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import Dataset\n",
    "\n",
    "def prepare_data(df):\n",
    "    data = []\n",
    "    for _, row in df.iterrows():\n",
    "        data.append({\n",
    "            \"input\": row[\"question\"],\n",
    "            \"output\": row[\"chain_of_thought\"] + \" \" + row[\"answer\"]\n",
    "        })\n",
    "    return data\n",
    "\n",
    "# Convert CSV data into JSON structure\n",
    "train_data = prepare_data(train_df)\n",
    "val_data = prepare_data(val_df)\n",
    "test_data = prepare_data(test_df)\n",
    "\n",
    "# Save JSON files for future use\n",
    "with open(\"train_data.json\", \"w\") as f:\n",
    "    json.dump(train_data, f, indent=4)\n",
    "with open(\"val_data.json\", \"w\") as f:\n",
    "    json.dump(val_data, f, indent=4)\n",
    "with open(\"test_data.json\", \"w\") as f:\n",
    "    json.dump(test_data, f, indent=4)\n",
    "\n",
    "# Convert to Hugging Face Dataset\n",
    "train_dataset = Dataset.from_list(train_data)\n",
    "val_dataset = Dataset.from_list(val_data)\n",
    "test_dataset = Dataset.from_list(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6b2ac4f-5f29-420d-8057-94a0eddb6fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Login\n",
    "hf_token = \"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\"  # Replace with actual token\n",
    "login(token=hf_token)\n",
    "\n",
    "# Load Gemma model\n",
    "model_name = \"google/gemma-3-1b-it\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1023f8fa-f264-4364-a6dc-f855a4c495c3",
   "metadata": {
    "id": "1023f8fa-f264-4364-a6dc-f855a4c495c3",
    "outputId": "33679d27-e409-4c17-e31d-28234acad194"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 745,472 || all params: 1,000,631,424 || trainable%: 0.0745\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "import torch\n",
    "\n",
    "model_id = \"google/gemma-3-4b-it\"\n",
    "\n",
    "model_name = \"google/gemma-3-1b-it\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"eager\"\n",
    ")\n",
    "base_model.gradient_checkpointing_enable()\n",
    "base_model = prepare_model_for_kbit_training(base_model)\n",
    "\n",
    "# Apply LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "model = get_peft_model(base_model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f98d42b-4930-4390-ab2e-7a2625bae35f",
   "metadata": {
    "id": "1f98d42b-4930-4390-ab2e-7a2625bae35f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/5988 [00:00<?, ? examples/s]/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3980: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "Map: 100%|| 5988/5988 [00:01<00:00, 4766.17 examples/s]\n",
      "Map: 100%|| 1497/1497 [00:00<00:00, 5909.05 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def tokenize_fn(example):\n",
    "    # Build a single prompt + target string\n",
    "    # (for causal-LM you can also concatenate input + output then create labels)\n",
    "    model_input = tokenizer(\n",
    "        example[\"input\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512,\n",
    "    )\n",
    "    with tokenizer.as_target_tokenizer():                       # old API\n",
    "        target = tokenizer(\n",
    "            example[\"output\"],\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=512,\n",
    "        )\n",
    "\n",
    "    model_input[\"labels\"] = target[\"input_ids\"]                 #  trainer looks for this\n",
    "    return model_input\n",
    "\n",
    "train_tok = train_dataset.map(tokenize_fn, batched=True, remove_columns=[\"input\", \"output\"])\n",
    "val_tok   = val_dataset.map(tokenize_fn,   batched=True, remove_columns=[\"input\", \"output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21789597-09a5-4007-bca1-72a13a6535ac",
   "metadata": {
    "id": "21789597-09a5-4007-bca1-72a13a6535ac",
    "outputId": "195de2ee-0527-45ef-d7a8-5141ba02c4c5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|| 5988/5988 [00:01<00:00, 5435.57 examples/s]\n",
      "Map: 100%|| 1497/1497 [00:00<00:00, 5977.89 examples/s]\n",
      "/tmp/ipykernel_10308/345464544.py:60: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='14970' max='14970' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [14970/14970 41:29, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.911300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.694900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>2.274300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.084800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.884000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.999000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>2.033200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.851200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>1.857000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.853400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>1.930900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.835900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>1.745900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.812000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>1.848800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.766200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>1.815700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.714500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>1.670600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.830800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>1.842900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>1.773300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>1.774200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.751700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>1.613900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>1.657200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>1.666600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>1.689700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>1.762300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.716400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>1.724000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>1.681000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>1.756100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>1.605000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>1.728100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>1.638900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>1.634400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>1.619600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>1.711200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.617000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>1.734300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>1.614200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>1.661800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>1.648000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>1.656600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>1.514800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>1.593300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>1.649200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>1.635200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.619800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>1.515400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>1.679600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>1.637100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>1.700100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>1.507300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>1.654000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2850</td>\n",
       "      <td>1.755300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>1.660900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2950</td>\n",
       "      <td>1.582100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.552500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3050</td>\n",
       "      <td>1.602700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>1.571600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3150</td>\n",
       "      <td>1.598800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>1.579700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3250</td>\n",
       "      <td>1.572700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>1.671000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3350</td>\n",
       "      <td>1.630300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>1.614800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3450</td>\n",
       "      <td>1.648700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>1.619200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3550</td>\n",
       "      <td>1.665000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>1.585200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3650</td>\n",
       "      <td>1.581300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>1.568900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3750</td>\n",
       "      <td>1.575300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>1.582700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3850</td>\n",
       "      <td>1.664200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>1.498900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3950</td>\n",
       "      <td>1.586400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.616100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4050</td>\n",
       "      <td>1.638100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>1.716500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4150</td>\n",
       "      <td>1.686300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>1.565600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4250</td>\n",
       "      <td>1.588300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>1.670000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4350</td>\n",
       "      <td>1.626900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>1.559200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4450</td>\n",
       "      <td>1.677700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>1.591100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4550</td>\n",
       "      <td>1.542300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>1.508900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4650</td>\n",
       "      <td>1.552300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>1.517500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4750</td>\n",
       "      <td>1.517400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>1.521500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4850</td>\n",
       "      <td>1.583800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>1.661700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4950</td>\n",
       "      <td>1.531500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>1.602100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5050</td>\n",
       "      <td>1.570200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>1.534400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5150</td>\n",
       "      <td>1.554200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>1.512600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5250</td>\n",
       "      <td>1.613000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>1.594700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5350</td>\n",
       "      <td>1.595000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>1.607000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5450</td>\n",
       "      <td>1.480300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>1.613600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5550</td>\n",
       "      <td>1.571300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>1.555600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5650</td>\n",
       "      <td>1.567100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5700</td>\n",
       "      <td>1.572000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5750</td>\n",
       "      <td>1.604100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>1.604400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5850</td>\n",
       "      <td>1.546600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5900</td>\n",
       "      <td>1.598400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5950</td>\n",
       "      <td>1.519200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>1.494200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6050</td>\n",
       "      <td>1.483900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6100</td>\n",
       "      <td>1.636300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6150</td>\n",
       "      <td>1.581000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>1.591500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6250</td>\n",
       "      <td>1.543900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6300</td>\n",
       "      <td>1.563600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6350</td>\n",
       "      <td>1.596500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>1.594400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6450</td>\n",
       "      <td>1.548100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>1.524300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6550</td>\n",
       "      <td>1.567000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>1.563900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6650</td>\n",
       "      <td>1.494100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6700</td>\n",
       "      <td>1.509700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6750</td>\n",
       "      <td>1.571500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>1.600900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6850</td>\n",
       "      <td>1.510800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6900</td>\n",
       "      <td>1.475100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6950</td>\n",
       "      <td>1.510500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>1.582700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7050</td>\n",
       "      <td>1.512400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7100</td>\n",
       "      <td>1.634500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7150</td>\n",
       "      <td>1.543500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>1.547400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7250</td>\n",
       "      <td>1.565800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7300</td>\n",
       "      <td>1.502200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7350</td>\n",
       "      <td>1.504000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>1.526400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7450</td>\n",
       "      <td>1.570200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>1.626200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7550</td>\n",
       "      <td>1.532600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>1.569100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7650</td>\n",
       "      <td>1.586000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7700</td>\n",
       "      <td>1.526600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7750</td>\n",
       "      <td>1.486200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>1.575800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7850</td>\n",
       "      <td>1.470700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7900</td>\n",
       "      <td>1.560600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7950</td>\n",
       "      <td>1.509100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>1.477000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8050</td>\n",
       "      <td>1.450900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8100</td>\n",
       "      <td>1.571300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8150</td>\n",
       "      <td>1.565800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8200</td>\n",
       "      <td>1.531000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8250</td>\n",
       "      <td>1.563400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8300</td>\n",
       "      <td>1.472000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8350</td>\n",
       "      <td>1.578800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8400</td>\n",
       "      <td>1.529100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8450</td>\n",
       "      <td>1.555400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>1.517500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8550</td>\n",
       "      <td>1.628400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8600</td>\n",
       "      <td>1.578900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8650</td>\n",
       "      <td>1.516900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8700</td>\n",
       "      <td>1.507500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8750</td>\n",
       "      <td>1.565900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8800</td>\n",
       "      <td>1.529800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8850</td>\n",
       "      <td>1.563500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8900</td>\n",
       "      <td>1.561200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8950</td>\n",
       "      <td>1.487400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>1.469000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9050</td>\n",
       "      <td>1.534700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9100</td>\n",
       "      <td>1.654000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9150</td>\n",
       "      <td>1.424400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9200</td>\n",
       "      <td>1.559700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9250</td>\n",
       "      <td>1.614200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9300</td>\n",
       "      <td>1.634000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9350</td>\n",
       "      <td>1.515400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9400</td>\n",
       "      <td>1.461400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9450</td>\n",
       "      <td>1.634800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>1.575200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9550</td>\n",
       "      <td>1.427900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9600</td>\n",
       "      <td>1.518700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9650</td>\n",
       "      <td>1.581400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9700</td>\n",
       "      <td>1.518000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9750</td>\n",
       "      <td>1.578800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9800</td>\n",
       "      <td>1.575900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9850</td>\n",
       "      <td>1.485600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9900</td>\n",
       "      <td>1.529700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9950</td>\n",
       "      <td>1.600400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>1.529500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10050</td>\n",
       "      <td>1.585100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10100</td>\n",
       "      <td>1.562400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10150</td>\n",
       "      <td>1.452500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10200</td>\n",
       "      <td>1.539600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10250</td>\n",
       "      <td>1.501000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10300</td>\n",
       "      <td>1.544100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10350</td>\n",
       "      <td>1.549200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10400</td>\n",
       "      <td>1.473300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10450</td>\n",
       "      <td>1.563100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>1.579500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10550</td>\n",
       "      <td>1.526100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10600</td>\n",
       "      <td>1.484400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10650</td>\n",
       "      <td>1.417400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10700</td>\n",
       "      <td>1.537200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10750</td>\n",
       "      <td>1.591900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10800</td>\n",
       "      <td>1.489900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10850</td>\n",
       "      <td>1.570000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10900</td>\n",
       "      <td>1.548700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10950</td>\n",
       "      <td>1.479600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>1.488000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11050</td>\n",
       "      <td>1.514600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11100</td>\n",
       "      <td>1.542300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11150</td>\n",
       "      <td>1.459900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11200</td>\n",
       "      <td>1.522000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11250</td>\n",
       "      <td>1.517700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11300</td>\n",
       "      <td>1.454100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11350</td>\n",
       "      <td>1.473500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11400</td>\n",
       "      <td>1.405100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11450</td>\n",
       "      <td>1.504900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>1.521600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11550</td>\n",
       "      <td>1.566800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11600</td>\n",
       "      <td>1.493000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11650</td>\n",
       "      <td>1.436000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11700</td>\n",
       "      <td>1.559600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11750</td>\n",
       "      <td>1.625000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11800</td>\n",
       "      <td>1.457500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11850</td>\n",
       "      <td>1.522700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11900</td>\n",
       "      <td>1.519400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11950</td>\n",
       "      <td>1.495100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>1.487400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12050</td>\n",
       "      <td>1.523100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12100</td>\n",
       "      <td>1.553700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12150</td>\n",
       "      <td>1.445400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12200</td>\n",
       "      <td>1.514400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12250</td>\n",
       "      <td>1.512800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12300</td>\n",
       "      <td>1.467000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12350</td>\n",
       "      <td>1.521900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12400</td>\n",
       "      <td>1.521700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12450</td>\n",
       "      <td>1.536600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>1.497600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12550</td>\n",
       "      <td>1.445100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12600</td>\n",
       "      <td>1.532400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12650</td>\n",
       "      <td>1.480000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12700</td>\n",
       "      <td>1.509600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12750</td>\n",
       "      <td>1.522700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12800</td>\n",
       "      <td>1.576400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12850</td>\n",
       "      <td>1.537700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12900</td>\n",
       "      <td>1.475600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12950</td>\n",
       "      <td>1.517300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>1.469700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13050</td>\n",
       "      <td>1.585100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13100</td>\n",
       "      <td>1.493300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13150</td>\n",
       "      <td>1.469600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13200</td>\n",
       "      <td>1.558700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13250</td>\n",
       "      <td>1.468100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13300</td>\n",
       "      <td>1.497500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13350</td>\n",
       "      <td>1.496200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13400</td>\n",
       "      <td>1.530500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13450</td>\n",
       "      <td>1.532400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>1.519500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13550</td>\n",
       "      <td>1.422200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13600</td>\n",
       "      <td>1.401400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13650</td>\n",
       "      <td>1.556500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13700</td>\n",
       "      <td>1.438800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13750</td>\n",
       "      <td>1.512600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13800</td>\n",
       "      <td>1.462100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13850</td>\n",
       "      <td>1.548800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13900</td>\n",
       "      <td>1.433700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13950</td>\n",
       "      <td>1.477000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>1.513900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14050</td>\n",
       "      <td>1.499500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14100</td>\n",
       "      <td>1.581600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14150</td>\n",
       "      <td>1.550600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14200</td>\n",
       "      <td>1.449600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14250</td>\n",
       "      <td>1.471600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14300</td>\n",
       "      <td>1.477400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14350</td>\n",
       "      <td>1.527100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14400</td>\n",
       "      <td>1.585500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14450</td>\n",
       "      <td>1.551800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>1.506400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14550</td>\n",
       "      <td>1.584500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14600</td>\n",
       "      <td>1.503700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14650</td>\n",
       "      <td>1.534500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14700</td>\n",
       "      <td>1.600100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14750</td>\n",
       "      <td>1.511200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14800</td>\n",
       "      <td>1.542100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14850</td>\n",
       "      <td>1.537200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14900</td>\n",
       "      <td>1.556100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14950</td>\n",
       "      <td>1.492200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=14970, training_loss=1.5867003292422655, metrics={'train_runtime': 2490.4258, 'train_samples_per_second': 12.022, 'train_steps_per_second': 6.011, 'total_flos': 6.425803034984448e+16, 'train_loss': 1.5867003292422655, 'epoch': 5.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import (\n",
    "    TrainingArguments, Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "import os, datasets\n",
    "\n",
    "# 0  make sure pad-token is set\n",
    "tokenizer.pad_token = tokenizer.eos_token          #  transformers 4.2 fix\n",
    "\n",
    "# 1  tokenization fn  returns input_ids / attention_mask / labels\n",
    "def tokenize_fn(batch):\n",
    "    # encode the prompt\n",
    "    prompt_enc = tokenizer(\n",
    "        batch[\"input\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512,\n",
    "    )\n",
    "    # encode the target / answer\n",
    "    target_enc = tokenizer(\n",
    "        batch[\"output\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512,\n",
    "    )\n",
    "    prompt_enc[\"labels\"] = target_enc[\"input_ids\"]  # Trainer looks for this\n",
    "    return prompt_enc\n",
    "\n",
    "# 2  map over the datasets and REMOVE the original text columns\n",
    "train_tok = train_dataset.map(tokenize_fn, batched=True,\n",
    "                              remove_columns=[\"input\", \"output\"])\n",
    "val_tok   = val_dataset.map(tokenize_fn,   batched=True,\n",
    "                            remove_columns=[\"input\", \"output\"])\n",
    "\n",
    "# 3  collator for causal-LM (no masked-LM)\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=False\n",
    ")\n",
    "\n",
    "# 4  training arguments that old APIs accept\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./gemma3\",\n",
    "    overwrite_output_dir=True,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=5,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    fp16=True,\n",
    "    logging_steps=50,\n",
    "    save_steps=100,\n",
    "    eval_steps=100,\n",
    "    save_total_limit=2,\n",
    "    remove_unused_columns=False,   # keep the keys we just created\n",
    "    report_to=\"none\",              # suppress WANDB warning\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_tok,\n",
    "    eval_dataset=val_tok,\n",
    "    tokenizer=tokenizer,           # still OK in older releases\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ada2bd5-d1c7-4563-a378-424ab27c80c5",
   "metadata": {
    "id": "3ada2bd5-d1c7-4563-a378-424ab27c80c5",
    "outputId": "349b226e-5d55-4ce7-b3ad-611df58ab71e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2/2 [00:08<00:00,  4.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---\n",
      "Question:\n",
      " How many vertical asymptotes does the graph of $y=\\frac{2}{x^2+x-6}$ have?\n",
      "\n",
      "Generated:\n",
      " How many vertical asymptotes does the graph of $y=\\frac{2}{x^2+x-6}$ have?\n",
      "<think>What is the sum of the values for each vertical asymptote?</think>.\n",
      "For example, if $\\lim_{x\\to 0} \\frac{2}{x^2+x-6}=1$, then we would say that there are two vertical asymptotes. If $\\lim_{x\\to -3} \\frac{2}{x^2+x-6}=-4$, then we would say that there were three vertical asymptotes.\n",
      "\n",
      "Final Answer: The final answer is $\\boxed{5}$\n",
      "\n",
      "Reference:\n",
      " The denominator of the rational function factors into $x^2+x-6=(x-2)(x+3)$. Since the numerator is always nonzero, there is a vertical asymptote whenever the denominator is $0$, which occurs for $x = 2$ and $x = -3$.  Therefore, the graph has $\\boxed{2}$ vertical asymptotes. Hence the answer to this question is $2$.\n",
      "\n",
      "---\n",
      "Question:\n",
      " What is the positive difference between $120\\%$ of 30 and $130\\%$ of 20?\n",
      "\n",
      "Generated:\n",
      " What is the positive difference between $120\\%$ of 30 and $130\\%$ of 20?\n",
      "<think>\n",
      "\n",
      "Let's find a number that can be written in the form $\\frac{x}{y}$ where $x$ is an integer, $y$ is also an integer, and $xy = 120$.  If we take $x=6$, then $y=5$. If we take $x=7$, then $y=4$.   The question asks for the \"positive difference\" between these two numbers. Let us consider what happens if $x=9$ and $y=8$: $9 \\times 8 = 72$. Also let us look at when $x=10$ and $y=10$. Then $10 \\times 10 = 100$. But this does not solve our problem!\n",
      "\n",
      "Final Answer: The final answer is $\\boxed{10}$\n",
      "\n",
      "Reference:\n",
      " One hundred twenty percent of 30 is $120\\cdot30\\cdot\\frac{1}{100}=36$, and $130\\%$ of 20 is $ 130\\cdot 20\\cdot\\frac{1}{100}=26$.  The difference between 36 and 26 is $\\boxed{10}$. Hence the answer to this question is $10$.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from transformers import GenerationConfig\n",
    "import torch\n",
    "\n",
    "# Make sure generation_config exists\n",
    "model.generation_config = GenerationConfig.from_pretrained(model_id)\n",
    "\n",
    "# Sample a couple of test instances\n",
    "sample_test = test_dataset.select(range(2))   # first 2 rows\n",
    "model.eval().to(\"cuda\")\n",
    "\n",
    "outputs = []\n",
    "for question in tqdm(sample_test[\"input\"]):\n",
    "    # Build the prompt with <think> tag + original question\n",
    "    prompt = f\"{question}\\n<think>\"\n",
    "\n",
    "    enc = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(\n",
    "            input_ids=enc[\"input_ids\"],\n",
    "            attention_mask=enc[\"attention_mask\"],\n",
    "            max_new_tokens=300,\n",
    "            do_sample=True,\n",
    "            temperature=0.5,\n",
    "            top_p=0.95,\n",
    "            repetition_penalty=1.2,\n",
    "            pad_token_id=tokenizer.eos_token_id,  # silence warning\n",
    "        )\n",
    "\n",
    "    # decode the same variable we just produced\n",
    "    decoded = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "    outputs.append(decoded)\n",
    "\n",
    "# Show side-by-side\n",
    "for q, gen, ref in zip(sample_test[\"input\"], outputs, sample_test[\"output\"]):\n",
    "    print(\"\\n---\")\n",
    "    print(\"Question:\\n\", q)\n",
    "    print(\"\\nGenerated:\\n\", gen)\n",
    "    print(\"\\nReference:\\n\", ref)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3b11fb29-ddf4-4bc7-b672-386bf5777adc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 10/10 [02:05<00:00, 12.55s/it]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from tqdm import tqdm\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "# Set generation config\n",
    "model.generation_config = GenerationConfig.from_pretrained(model_id)\n",
    "\n",
    "# Sample 50 random indices\n",
    "indices = random.sample(range(len(test_dataset)), 10)\n",
    "\n",
    "# Collect results\n",
    "generated_outputs = []\n",
    "\n",
    "model.eval()\n",
    "model.to(\"cuda\")\n",
    "\n",
    "for idx in tqdm(indices):\n",
    "    sample = test_dataset[idx]\n",
    "\n",
    "    question = sample[\"input\"]\n",
    "    reference_reasoning = sample[\"output\"]  #  use this as ground truth\n",
    "\n",
    "    # Prompt with <think> tag\n",
    "    input_text = f\"Question: {question}\\n\\n<think>\\n\"\n",
    "\n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(\n",
    "        input_text,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )\n",
    "    input_ids = inputs[\"input_ids\"].to(\"cuda\")\n",
    "    attention_mask = inputs[\"attention_mask\"].to(\"cuda\")\n",
    "\n",
    "    # Generate output\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=500,\n",
    "            do_sample=True,\n",
    "            temperature=0.5,\n",
    "            top_p=0.95,\n",
    "            repetition_penalty=1.2\n",
    "        )\n",
    "\n",
    "    decoded_output = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    # Store for evaluation\n",
    "    generated_outputs.append({\n",
    "        \"question\": question,\n",
    "        \"generated\": decoded_output,\n",
    "        \"reference\": reference_reasoning\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "623af8cb-70ab-4091-8a05-d378809ff3d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# In a notebook cell:\n",
    "!pip install --quiet --upgrade evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6d4755a7-350f-4dea-b3f7-a0f03adf5a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, datasets\n",
    "from datasets import Dataset\n",
    "from transformers import GenerationConfig\n",
    "import torch, evaluate\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "N = 10\n",
    "rand_idx = random.sample(range(len(test_dataset)), N)\n",
    "eval_ds  = test_dataset.select(rand_idx)     # columns: \"input\", \"output\"\n",
    "answers  = list(eval_ds[\"output\"])           # ground-truth chain + final answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "29820038-f86c-45c7-b9f6-b8056a382069",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device).eval()\n",
    "\n",
    "def generate_one(prompt,\n",
    "                 max_new=256,\n",
    "                 temperature=0.7,\n",
    "                 top_p=0.95,\n",
    "                 seed=None,\n",
    "                 sample=True):                 #  new flag\n",
    "    gcfg = GenerationConfig(\n",
    "        do_sample=sample,\n",
    "        temperature=max(1e-5, temperature),   # clamp to >0\n",
    "        top_p=top_p,\n",
    "        max_new_tokens=max_new,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "    toks = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    out_ids = model.generate(**toks, generation_config=gcfg)[0]\n",
    "    return tokenizer.decode(out_ids, skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2e5c5728-ad14-4feb-ad6b-b98acdb891df",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPTS = dict(\n",
    "    # 3.1 Chain-of-Draft\n",
    "    cod=\n",
    "\"\"\"<system>You are an expert food reasoner. Break the problem into VERY SHORT drafts (5 words each), then give the final answer.</system>\n",
    "\n",
    "<question>{q}</question>\n",
    "<draft>\n",
    "\"\"\",\n",
    "\n",
    "    # 3.2 Plain Chain-of-Thought (baseline for self-consistency)\n",
    "    cot=\n",
    "\"\"\"<system>Think step-by-step.</system>\n",
    "<question>{q}</question>\n",
    "<thought>\n",
    "\"\"\",\n",
    "\n",
    "    # 3.3 Tree-of-Thought (well embed the root prompt; ToT search loop below)\n",
    "    tot_root=\n",
    "\"\"\"You are an agent who can think in branches.\n",
    "Problem: {q}\n",
    "Start with an initial thought.\n",
    "\"\"\",\n",
    "\n",
    "    # 3.4 GRPO: force pseudo-code blocks\n",
    "    grpo=\n",
    "\"\"\"<system>Where helpful, write python-like code blocks to compute intermediate results, then print the answer.</system>\n",
    "Q: {q}\n",
    "A:\n",
    "\"\"\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7bcee82c-58eb-468a-a7a5-5f82fe60a023",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cod(q):\n",
    "    return generate_one(PROMPTS[\"cod\"].format(q=q))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0342ec85-8769-4353-a386-ab28802eb44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def run_self_consistency(q, K=10):\n",
    "    samples = [\n",
    "        generate_one(PROMPTS[\"cot\"].format(q=q), seed=42+i)\n",
    "        for i in range(K)\n",
    "    ]\n",
    "    # extract final line (after last newline) as the predicted answer\n",
    "    finals = [s.strip().splitlines()[-1] for s in samples]\n",
    "    return Counter(finals).most_common(1)[0][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "abed5742-3ec1-46d7-a008-da01d8385f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand(node_text, n=2):\n",
    "    return [generate_one(node_text + f\"\\nThought {i+1}:\", seed=123+i)\n",
    "            for i in range(n)]\n",
    "\n",
    "def score(text):\n",
    "    judge_prompt = text + \"\\nIs this answer correct? Answer yes or no.\"\n",
    "    # greedy decode  sample=False, temperature ignored\n",
    "    out = generate_one(judge_prompt,\n",
    "                       max_new=5,\n",
    "                       temperature=1.0,   # any value now OK\n",
    "                       sample=False)       #  deterministic\n",
    "    return 1 if \"yes\" in out.lower() else 0\n",
    "\n",
    "def run_tot(q):\n",
    "    root = PROMPTS[\"tot_root\"].format(q=q)\n",
    "    level1 = expand(root, n=2)\n",
    "    best1  = max(level1, key=score)\n",
    "    level2 = expand(best1, n=2)\n",
    "    best2  = max(level2, key=score)\n",
    "    return best2.strip().splitlines()[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e3a83d1d-de38-41a0-9d9e-c42e54cd4c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_grpo(q):\n",
    "    return generate_one(PROMPTS[\"grpo\"].format(q=q))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3b254361-5a82-42c8-99bd-42f5a8a5f1bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 10/10 [20:36<00:00, 123.67s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "tech2func = dict(cod=run_cod,\n",
    "                 selfc=run_self_consistency,\n",
    "                 tot=run_tot,\n",
    "                 grpo=run_grpo)\n",
    "\n",
    "preds = {name: [] for name in tech2func}\n",
    "\n",
    "for q in tqdm(eval_ds[\"input\"]):\n",
    "    for name, func in tech2func.items():\n",
    "        preds[name].append(func(q))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2b69d823-3f0e-4d93-8910-bc3e5ee36bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.51.3)\n",
      "Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (1.6.0)\n",
      "Requirement already satisfied: bitsandbytes in /opt/conda/lib/python3.10/site-packages (0.45.5)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.31.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.1.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2023.12.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2025.4.26)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=2.0.0->accelerate) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=2.0.0->accelerate) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -U transformers accelerate bitsandbytes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c79ae255-56a6-45dc-9599-1199d0522293",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "JUDGE_ID = \"amazingvince/zephyr-1.1b-sft-full\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    llm_int8_enable_fp32_cpu_offload=True\n",
    ")\n",
    "\n",
    "judge_tok = AutoTokenizer.from_pretrained(JUDGE_ID, use_fast=True)\n",
    "\n",
    "judge_mod = AutoModelForCausalLM.from_pretrained(\n",
    "    JUDGE_ID,\n",
    "    device_map=None,  #  Change this\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# Force init of quant state (critical fix)\n",
    "judge_mod = judge_mod.to(\"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "505518d9-e1b9-47d2-aa4f-f0b6f28ba210",
   "metadata": {},
   "outputs": [],
   "source": [
    "JUDGE_SYSTEM = (\n",
    "    \"Evaluate the chain-of-thought reasoning in the generated text compared to the reference. \"\n",
    "    \"Assess whether the reasoning steps are logical, complete, and lead to the correct conclusion.\"\n",
    ")\n",
    "\n",
    "def single_score(pred, ref):\n",
    "    prompt = (\n",
    "        f\"{JUDGE_SYSTEM}\\n\\n\"\n",
    "        f\"### Reference Answer:\\n{ref}\\n\\n\"\n",
    "        f\"### Candidate Answer:\\n{pred}\\n\\n\"\n",
    "        f\"### Rate the candidate on a scale of 0 to 10.\\nScore:\"\n",
    "    )\n",
    "\n",
    "    inputs = judge_tok(prompt, return_tensors=\"pt\", truncation=True, max_length=768).to(judge_mod.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = judge_mod.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=16,\n",
    "            do_sample=False,\n",
    "            temperature=0.5,   # or omit completely since do_sample=False\n",
    "            pad_token_id=judge_tok.eos_token_id\n",
    "        )\n",
    "\n",
    "    decoded = judge_tok.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # Extract last number\n",
    "    try:\n",
    "        score = int([s for s in decoded.strip().split() if s.isdigit()][-1])\n",
    "    except:\n",
    "        score = 0\n",
    "\n",
    "    return max(0, min(10, score))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "71f3aa3e-1aad-427b-bf64-392fa998b9d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from statistics import mean\n",
    "import evaluate\n",
    "\n",
    "bleu      = evaluate.load(\"bleu\")\n",
    "meteor    = evaluate.load(\"meteor\")\n",
    "rouge     = evaluate.load(\"rouge\")\n",
    "chrf      = evaluate.load(\"chrf\")\n",
    "bertscore = evaluate.load(\"bertscore\")\n",
    "\n",
    "def g_eval_open(preds, refs):\n",
    "    scores = [single_score(p, r) for p, r in zip(preds, refs)]\n",
    "    return sum(scores) / len(scores)\n",
    "\n",
    "def calc_all(pred_list, refs):\n",
    "    bs = bertscore.compute(\n",
    "        predictions=pred_list,\n",
    "        references=refs,\n",
    "        model_type=\"bert-base-uncased\",\n",
    "    )[\"f1\"]\n",
    "\n",
    "    return dict(\n",
    "        bleu    = bleu.compute(predictions=pred_list,  references=refs)[\"bleu\"],\n",
    "        meteor  = meteor.compute(predictions=pred_list, references=refs)[\"meteor\"],\n",
    "        rougeL  = rouge.compute(predictions=pred_list,  references=refs)[\"rougeL\"],\n",
    "        chrf    = chrf.compute(predictions=pred_list,   references=refs)[\"score\"],\n",
    "        bert_f1 = mean(bs),\n",
    "        g_eval  = g_eval_open(pred_list, refs)          #  local judge\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5f66273c-f78c-4fa0-88c2-5e5a90947fdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/nn/modules.py:451: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "results = {name: calc_all(ps, answers) for name, ps in preds.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ebdbe919-b491-4f31-84ed-357c6b1ed9e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       g_eval  bert_f1    bleu  meteor  rougeL     chrf\n",
      "cod       1.3   0.5475  0.0536  0.1591  0.1382  23.3072\n",
      "grpo      1.3   0.5651  0.0537  0.1847  0.1433  25.9185\n",
      "selfc     0.0   0.3847  0.0000  0.0112  0.0327   1.9687\n",
      "tot       0.0   0.3935  0.0009  0.0381  0.0813   7.0165\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "leader = (pd.DataFrame(results)          # rows = technique, cols = metrics\n",
    "            .T                          # transpose so each technique is a row\n",
    "            .loc[:, [\"g_eval\", \"bert_f1\", \"bleu\", \"meteor\", \"rougeL\", \"chrf\"]]\n",
    "            .round(4)                   # 4-decimal precision\n",
    "            .sort_values(\"g_eval\", ascending=False))\n",
    "\n",
    "print(leader.to_string())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "24ff5c58-7466-435f-9fe2-5570e9fee00f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Updated data with new values\n",
    "data = {\n",
    "    \"technique\": [\"COD\", \"GRPO\", \"Self-Consistency\", \"ToT\"],\n",
    "    \"g_eval\":    [1.3, 1.3, 0.0, 0.0],\n",
    "    \"bert_f1\":   [0.5475, 0.5651, 0.3847, 0.3935],\n",
    "    \"bleu\":      [0.0536, 0.0537, 0.0000, 0.0009],\n",
    "    \"meteor\":    [0.1591, 0.1847, 0.0112, 0.0381],\n",
    "    \"rougeL\":    [0.1382, 0.1433, 0.0327, 0.0813]\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data).set_index(\"technique\")\n",
    "\n",
    "# Plot grouped bar chart\n",
    "ax = df.plot.bar(figsize=(12, 6))\n",
    "ax.set_ylabel(\"Score\")\n",
    "ax.set_title(\"Evaluation Metrics by Reasoning Technique\")\n",
    "ax.set_xticklabels(df.index, rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618a5d53-1ea7-412b-baaa-e209d6d67022",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd262d89-82c4-4147-8ed3-5bcfad13fb93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
